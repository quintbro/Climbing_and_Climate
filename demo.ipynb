{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclimb as pc\n",
    "from pyclimb.vis_func import clus_map, heat_map\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Cleaning\n",
    "First, use the load_data function to load in the preloaded datasets for the demo. The preloaded datasets include the climbing dataset both in clean, and in raw form, a weather dataset from Utah weather stations, and a Utah cities dataset scraped from the web. For this demo, we are using the raw form of the data set, to demonstrate how to use the built in cleaning function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If reading in your own data from mountainproject.com. Put all of your files in the same working directory and use the pc.concat() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using your own files dowloaded from mountain project,then uncomment this code\n",
    "# climbs = pc.concat(['route-finder(1).csv', 'route-finder(2).csv', 'route-finder(3).csv'])\n",
    "climbs = pc.load_data('raw')\n",
    "weather = pc.load_data('weather')\n",
    "cities = pc.load_data('cities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then clean the data using the pc.clean() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.clean(climbs, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "Additionally, there is a scraper function that will collect additional data from each climb, the crawl-delay requested by mountain project.com, is 60 seconds, so for the purposes of this demo, I will leave it commented out, but if you are using your own data feel free to uncomment it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc.scrape_mp(climbs, inplace = True) # uncomment this section to scrape from MP\n",
    "# I have already scraped the data from MP and it is in the clean dataset below\n",
    "climbs = pc.load_data('clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the data\n",
    "Once you have your cleaned data, use merge_data_dist to merge the desired dataframes based on the closest latitude and longitude. This example merges the climbs dataset with two others, weather stations and cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6107 entries, 0 to 6106\n",
      "Data columns (total 34 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Route          6107 non-null   object \n",
      " 1   URL            6107 non-null   object \n",
      " 2   Avg Stars      6062 non-null   float64\n",
      " 3   Rating         6107 non-null   object \n",
      " 4   Pitches        6107 non-null   int64  \n",
      " 5   Length         5474 non-null   float64\n",
      " 6   Latitude       6107 non-null   float64\n",
      " 7   Longitude      6107 non-null   float64\n",
      " 8   PG13           6107 non-null   bool   \n",
      " 9   R              6107 non-null   bool   \n",
      " 10  State          6107 non-null   object \n",
      " 11  Region         6107 non-null   object \n",
      " 12  Location       6107 non-null   object \n",
      " 13  Crag           5874 non-null   object \n",
      " 14  Wall           4702 non-null   object \n",
      " 15  Trad           6107 non-null   bool   \n",
      " 16  Alpine         6107 non-null   bool   \n",
      " 17  TR             6107 non-null   bool   \n",
      " 18  Aid            6107 non-null   bool   \n",
      " 19  Boulder        6107 non-null   bool   \n",
      " 20  Mixed          6107 non-null   bool   \n",
      " 21  Rating_num     6107 non-null   float64\n",
      " 22  numVotes       6107 non-null   int64  \n",
      " 23  numViews       6107 non-null   int64  \n",
      " 24  Year           6107 non-null   int64  \n",
      " 25  ViewsPerMonth  6107 non-null   int64  \n",
      " 26  Shared_by      6107 non-null   object \n",
      " 27  Month          6107 non-null   int64  \n",
      " 28  Day            6107 non-null   int64  \n",
      " 29  Date           6107 non-null   object \n",
      " 30  STATION_NA     6107 non-null   object \n",
      " 31  Distance       6107 non-null   float64\n",
      " 32  Location       6107 non-null   object \n",
      " 33  Distance       6107 non-null   float64\n",
      "dtypes: bool(8), float64(7), int64(7), object(12)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "climbing = pc.merge_data_dist(climbs, weather, 'STATION_NA', 'Area Latitude', 'Area Longitude', 'LATITUDE', 'LONGITUDE')\n",
    "climbing = pc.merge_data_dist(climbing, cities, 'Location')\n",
    "climbing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Here we use a weighted linear regression to analyze the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
